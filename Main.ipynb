{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDOyuJzU7vt7",
        "outputId": "861647fe-1332-4a8d-a763-fec6ca3c297f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-414952608.py:90: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  trials_idx = trials_idx.squeeze().astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 1/8 ===\n",
            "Ep01: tr-loss 0.7293 | tr-acc 50.00% | early-val-acc 47.22%\n",
            "Ep05: tr-loss 0.6827 | tr-acc 56.75% | early-val-acc 52.78%\n",
            "Ep10: tr-loss 0.6961 | tr-acc 48.41% | early-val-acc 55.56%\n",
            "Ep15: tr-loss 0.6974 | tr-acc 50.00% | early-val-acc 69.44%\n",
            "Ep20: tr-loss 0.6697 | tr-acc 59.92% | early-val-acc 58.33%\n",
            "Ep25: tr-loss 0.6759 | tr-acc 60.71% | early-val-acc 61.11%\n",
            "Ep30: tr-loss 0.6144 | tr-acc 68.25% | early-val-acc 50.00%\n",
            "Ep35: tr-loss 0.6041 | tr-acc 68.25% | early-val-acc 58.33%\n",
            "♦ Fold 1 final early-800ms acc: 69.44%\n",
            "\n",
            "=== Fold 2/8 ===\n",
            "Ep01: tr-loss 0.7396 | tr-acc 48.02% | early-val-acc 38.89%\n",
            "Ep05: tr-loss 0.6927 | tr-acc 51.59% | early-val-acc 38.89%\n",
            "Ep10: tr-loss 0.6954 | tr-acc 50.00% | early-val-acc 47.22%\n",
            "Ep15: tr-loss 0.5177 | tr-acc 76.19% | early-val-acc 38.89%\n",
            "Ep20: tr-loss 0.5410 | tr-acc 75.00% | early-val-acc 47.22%\n",
            "Ep25: tr-loss 0.2994 | tr-acc 88.89% | early-val-acc 58.33%\n",
            "Ep30: tr-loss 0.2908 | tr-acc 87.30% | early-val-acc 52.78%\n",
            "Ep35: tr-loss 0.3892 | tr-acc 81.75% | early-val-acc 44.44%\n",
            "♦ Fold 2 final early-800ms acc: 63.89%\n",
            "\n",
            "=== Fold 3/8 ===\n",
            "Ep01: tr-loss 0.7324 | tr-acc 48.81% | early-val-acc 38.89%\n",
            "Ep05: tr-loss 0.6952 | tr-acc 51.98% | early-val-acc 47.22%\n",
            "Ep10: tr-loss 0.6744 | tr-acc 53.97% | early-val-acc 44.44%\n",
            "Ep15: tr-loss 0.5528 | tr-acc 73.41% | early-val-acc 50.00%\n",
            "Ep20: tr-loss 0.5036 | tr-acc 78.97% | early-val-acc 41.67%\n",
            "Ep25: tr-loss 0.4027 | tr-acc 82.54% | early-val-acc 55.56%\n",
            "Ep30: tr-loss 0.3618 | tr-acc 86.51% | early-val-acc 50.00%\n",
            "Ep35: tr-loss 0.3444 | tr-acc 86.11% | early-val-acc 50.00%\n",
            "♦ Fold 3 final early-800ms acc: 63.89%\n",
            "\n",
            "=== Fold 4/8 ===\n",
            "Ep01: tr-loss 0.7159 | tr-acc 50.00% | early-val-acc 63.89%\n",
            "Ep05: tr-loss 0.6964 | tr-acc 48.02% | early-val-acc 50.00%\n",
            "Ep10: tr-loss 0.6611 | tr-acc 63.10% | early-val-acc 47.22%\n",
            "Ep15: tr-loss 0.6880 | tr-acc 53.97% | early-val-acc 41.67%\n",
            "Ep20: tr-loss 0.6831 | tr-acc 54.76% | early-val-acc 47.22%\n",
            "Ep25: tr-loss 0.6401 | tr-acc 65.87% | early-val-acc 50.00%\n",
            "Ep30: tr-loss 0.5734 | tr-acc 73.02% | early-val-acc 50.00%\n",
            "Ep35: tr-loss 0.6620 | tr-acc 62.70% | early-val-acc 44.44%\n",
            "♦ Fold 4 final early-800ms acc: 63.89%\n",
            "\n",
            "=== Fold 5/8 ===\n",
            "Ep01: tr-loss 0.7331 | tr-acc 49.21% | early-val-acc 55.56%\n",
            "Ep05: tr-loss 0.7097 | tr-acc 48.02% | early-val-acc 55.56%\n",
            "Ep10: tr-loss 0.6624 | tr-acc 59.92% | early-val-acc 36.11%\n",
            "Ep15: tr-loss 0.5234 | tr-acc 75.79% | early-val-acc 44.44%\n",
            "Ep20: tr-loss 0.4006 | tr-acc 81.35% | early-val-acc 55.56%\n",
            "Ep25: tr-loss 0.2431 | tr-acc 92.06% | early-val-acc 50.00%\n",
            "Ep30: tr-loss 0.2468 | tr-acc 90.87% | early-val-acc 58.33%\n",
            "Ep35: tr-loss 0.1683 | tr-acc 94.05% | early-val-acc 52.78%\n",
            "♦ Fold 5 final early-800ms acc: 66.67%\n",
            "\n",
            "=== Fold 6/8 ===\n",
            "Ep01: tr-loss 0.7453 | tr-acc 48.81% | early-val-acc 50.00%\n",
            "Ep05: tr-loss 0.7083 | tr-acc 45.24% | early-val-acc 52.78%\n",
            "Ep10: tr-loss 0.6928 | tr-acc 51.59% | early-val-acc 44.44%\n",
            "Ep15: tr-loss 0.6905 | tr-acc 52.38% | early-val-acc 52.78%\n",
            "Ep20: tr-loss 0.6202 | tr-acc 67.86% | early-val-acc 50.00%\n",
            "Ep25: tr-loss 0.5830 | tr-acc 72.62% | early-val-acc 55.56%\n",
            "Ep30: tr-loss 0.5826 | tr-acc 71.03% | early-val-acc 55.56%\n",
            "Ep35: tr-loss 0.4547 | tr-acc 80.56% | early-val-acc 52.78%\n",
            "♦ Fold 6 final early-800ms acc: 61.11%\n",
            "\n",
            "=== Fold 7/8 ===\n",
            "Ep01: tr-loss 0.7405 | tr-acc 47.62% | early-val-acc 47.22%\n",
            "Ep05: tr-loss 0.7007 | tr-acc 52.78% | early-val-acc 47.22%\n",
            "Ep10: tr-loss 0.6934 | tr-acc 52.38% | early-val-acc 47.22%\n",
            "Ep15: tr-loss 0.6443 | tr-acc 64.29% | early-val-acc 50.00%\n",
            "Ep20: tr-loss 0.5703 | tr-acc 71.03% | early-val-acc 47.22%\n",
            "Ep25: tr-loss 0.4377 | tr-acc 79.76% | early-val-acc 44.44%\n",
            "Ep30: tr-loss 0.3075 | tr-acc 88.10% | early-val-acc 38.89%\n",
            "Ep35: tr-loss 0.2702 | tr-acc 88.89% | early-val-acc 58.33%\n",
            "♦ Fold 7 final early-800ms acc: 58.33%\n",
            "\n",
            "=== Fold 8/8 ===\n",
            "Ep01: tr-loss 0.7242 | tr-acc 48.41% | early-val-acc 47.22%\n",
            "Ep05: tr-loss 0.7152 | tr-acc 49.60% | early-val-acc 52.78%\n",
            "Ep10: tr-loss 0.6666 | tr-acc 58.33% | early-val-acc 47.22%\n",
            "Ep15: tr-loss 0.5098 | tr-acc 76.98% | early-val-acc 55.56%\n",
            "Ep20: tr-loss 0.3331 | tr-acc 85.71% | early-val-acc 38.89%\n",
            "Ep25: tr-loss 0.2188 | tr-acc 90.87% | early-val-acc 41.67%\n",
            "Ep30: tr-loss 0.1171 | tr-acc 96.03% | early-val-acc 44.44%\n",
            "Ep35: tr-loss 0.0669 | tr-acc 96.83% | early-val-acc 44.44%\n",
            "♦ Fold 8 final early-800ms acc: 63.89%\n",
            "\n",
            "Mean ± SD early-800ms accuracy over 8 folds: 63.89% ± 3.11%\n",
            "\n",
            "TRAIN  (all segments)\n",
            "---------------------\n",
            "Trials               : 288\n",
            "Segments             : 17369\n",
            "[CLS] tokens (1/trial): 288\n",
            "Total tokens         : 17657\n",
            "Segments/trial       : mean 60.3, min 22, max 103\n",
            "\n",
            "TEST   (segments ending ≤800 ms)\n",
            "--------------------------------\n",
            "Trials               : 288\n",
            "Segments             : 4569\n",
            "[CLS] tokens (1/trial): 288\n",
            "Total tokens         : 4857\n",
            "Segments/trial       : mean 15.9, min 5, max 30\n",
            "\n",
            "8-fold split sizes (train / valid per fold, ≤800 ms)\n",
            "Fold 1:  252 trials (15349 segs) /  36 trials ( 538 segs ≤800 ms)\n",
            "Fold 2:  252 trials (15097 segs) /  36 trials ( 597 segs ≤800 ms)\n",
            "Fold 3:  252 trials (15135 segs) /  36 trials ( 613 segs ≤800 ms)\n",
            "Fold 4:  252 trials (15231 segs) /  36 trials ( 525 segs ≤800 ms)\n",
            "Fold 5:  252 trials (15090 segs) /  36 trials ( 583 segs ≤800 ms)\n",
            "Fold 6:  252 trials (15262 segs) /  36 trials ( 575 segs ≤800 ms)\n",
            "Fold 7:  252 trials (15225 segs) /  36 trials ( 554 segs ≤800 ms)\n",
            "Fold 8:  252 trials (15194 segs) /  36 trials ( 584 segs ≤800 ms)\n",
            "\n",
            "CV Mean Statistics:\n",
            "CV Training Trials      : 259\n",
            "CV Training Segments    : 15632\n",
            "CV Testing Trials       : 29\n",
            "CV Testing Segments (≤800 ms) : 457\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"SE_transformer_best.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1o8XLVIFzTrY-_yrcbAWGiVU3Vzm9eTu-\n",
        "\"\"\"\n",
        "\n",
        "# ================================================================\n",
        "# Early-MI classification with SE-enhanced Transformer (mix T+E, 8-fold CV)\n",
        "# ================================================================\n",
        "\n",
        "import os, glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.io import loadmat\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 0) Reproducibility\n",
        "# --------------------------------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 1) Mount Drive  (comment out if running locally)\n",
        "# --------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 2) Dataset  (same code, but we’ll instantiate twice: full vs early)\n",
        "# --------------------------------------------------\n",
        "data_dir   = '/content/drive/My Drive/conferencedata'\n",
        "subject_id = 2\n",
        "tasks      = ['left','right']           # only these two MI classes\n",
        "\n",
        "class EEGGraphDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each trial → a sequence of connectivity-graph vectors (shape: T_i × 231), plus an int label.\n",
        "    If test_early=True, only keep segments whose segment_end_ms ≤ 800 ms.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, subject_id, test_early=False, early_ms=800):\n",
        "        self.trials, self.labels = [], []\n",
        "        task_to_lab  = {t: i for i, t in enumerate(tasks)}\n",
        "        subj_tag     = f\"A{subject_id:02d}\"\n",
        "        iu           = np.triu_indices(22, k=1)\n",
        "\n",
        "        for sess in ['T','E']:  # files from both training (“T”) and evaluation (“E”) sessions\n",
        "            pattern = Path(data_dir) / f\"connectivity_graphs_{subj_tag}{sess}_*.mat\"\n",
        "            for fpath in sorted(glob.glob(str(pattern))):\n",
        "                task = Path(fpath).stem.split('_')[-1].lower()\n",
        "                if task not in task_to_lab:\n",
        "                    continue\n",
        "                lab = task_to_lab[task]\n",
        "\n",
        "                mat = loadmat(fpath)\n",
        "                segs_mat       = mat['connectivity_graphs']\n",
        "                segment_end_ms = mat['segment_end_ms'].squeeze()\n",
        "                trials_idx     = mat.get('trial_indices', None)\n",
        "\n",
        "                # convert to list of 2D arrays, one per segment\n",
        "                if segs_mat.dtype == np.object_:\n",
        "                    segs = [np.asarray(g) for g in segs_mat.squeeze()]\n",
        "                else:\n",
        "                    segs = [segs_mat[:, :, i] for i in range(segs_mat.shape[2])]\n",
        "\n",
        "                def add(seq):\n",
        "                    stacked = np.stack([s[iu].astype(np.float32) for s in seq])\n",
        "                    self.trials.append(stacked)\n",
        "                    self.labels.append(lab)\n",
        "\n",
        "                if trials_idx is not None:\n",
        "                    trials_idx = trials_idx.squeeze().astype(int)\n",
        "                    for t in np.unique(trials_idx):\n",
        "                        mask    = (trials_idx == t)\n",
        "                        segs_t  = [segs[i] for i in np.where(mask)[0]]\n",
        "                        ends_t  = segment_end_ms[mask]\n",
        "                        if test_early:\n",
        "                            segs_t = [s for s, e in zip(segs_t, ends_t) if e <= 800]\n",
        "                        if segs_t:\n",
        "                            add(segs_t)\n",
        "                else:\n",
        "                    segs_f = [s for s, e in zip(segs, segment_end_ms)\n",
        "                              if (e <= 800 if test_early else True)]\n",
        "                    if segs_f:\n",
        "                        add(segs_f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trials)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.trials[idx], self.labels[idx]\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 3) Pad-collate  (unchanged)\n",
        "# --------------------------------------------------\n",
        "def pad_collate(batch):\n",
        "    seqs = [torch.tensor(x, dtype=torch.float32) for x, _ in batch]\n",
        "    labs = torch.tensor([y for _, y in batch], dtype=torch.long)\n",
        "    lengths = [s.shape[0] for s in seqs]\n",
        "    max_len = max(lengths)\n",
        "\n",
        "    padded = torch.stack([\n",
        "        torch.cat([s, torch.zeros(max_len - s.shape[0], s.shape[1])], dim=0)\n",
        "        for s in seqs\n",
        "    ])\n",
        "    pad_mask = torch.stack([\n",
        "        torch.tensor([False] * l + [True] * (max_len - l)) for l in lengths\n",
        "    ])\n",
        "    return padded, labs, pad_mask\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 4) Squeeze-and-Excitation block\n",
        "# --------------------------------------------------\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Treat d_model as 'channels'.  Input x shape: (B, T, d_model)\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, r=4):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_model // r, bias=False)\n",
        "        self.fc2 = nn.Linear(d_model // r, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, d_model)\n",
        "        s = x.mean(dim=1)                         # (B, d_model)\n",
        "        z = F.relu(self.fc1(s))                   # (B, d_model//r)\n",
        "        z = torch.sigmoid(self.fc2(z)).unsqueeze(1)  # (B, 1, d_model)\n",
        "        return x * z                              # broadcast → (B, T, d_model)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 5) Encoder layer with SE after FFN\n",
        "# --------------------------------------------------\n",
        "class SEEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            d_model, nhead, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        # feedforward network\n",
        "        self.ff1   = nn.Linear(d_model, dim_ff)\n",
        "        self.ff2   = nn.Linear(dim_ff, d_model)\n",
        "        self.se    = SEBlock(d_model)      # new SE block\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.drop  = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, key_padding_mask=None):\n",
        "        attn_out, _ = self.self_attn(\n",
        "            x, x, x, key_padding_mask=key_padding_mask, need_weights=False\n",
        "        )\n",
        "        x = self.norm1(x + self.drop(attn_out))\n",
        "\n",
        "        y = self.ff2(self.drop(F.gelu(self.ff1(x))))\n",
        "        y = self.se(y)                              # apply SE\n",
        "        x = self.norm2(x + self.drop(y))\n",
        "        return x\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 6) Transformer classifier\n",
        "# --------------------------------------------------\n",
        "class EEGTransformerClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_dim=231,\n",
        "        d_model=16,\n",
        "        nhead=2,\n",
        "        num_layers=2,\n",
        "        num_classes=2,\n",
        "        dim_ff=256,\n",
        "        dropout=0.1,\n",
        "        max_seq_len=100\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(feature_dim, d_model)\n",
        "        self.cls  = nn.Parameter(torch.zeros(1, 1, d_model))\n",
        "\n",
        "        # fixed sinusoidal positional embeddings\n",
        "        pos = torch.arange(max_seq_len + 1).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2) * -(np.log(1e4) / d_model))\n",
        "        pe  = torch.zeros(max_seq_len + 1, d_model)\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            SEEncoderLayer(d_model, nhead, dim_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc   = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x, pad_mask=None):\n",
        "        # x: (B, T, feature_dim)\n",
        "        B, T, _ = x.shape\n",
        "        x = self.proj(x)                           # → (B, T, d_model)\n",
        "        cls = self.cls.expand(B, -1, -1)           # → (B, 1, d_model)\n",
        "        x = torch.cat([cls, x], dim=1) + self.pe[:T + 1]\n",
        "        if pad_mask is not None:\n",
        "            pad_mask = torch.cat([pad_mask.new_zeros((B, 1)), pad_mask], dim=1)\n",
        "        for lyr in self.layers:\n",
        "            x = lyr(x, key_padding_mask=pad_mask)\n",
        "        # classify based on the [CLS] token output\n",
        "        return self.fc(self.drop(x[:, 0, :]))\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 7) Load “full” vs “early” datasets (mix T + E now)\n",
        "# --------------------------------------------------\n",
        "full_ds  = EEGGraphDataset(data_dir, subject_id, test_early=False)  # all segments\n",
        "early_ds = EEGGraphDataset(data_dir, subject_id, test_early=True)   # only ≤800 ms\n",
        "\n",
        "# Sanity check: both lists have same trial count & same labels in same order\n",
        "assert len(full_ds) == len(early_ds)\n",
        "assert all(a == b for a, b in zip(full_ds.labels, early_ds.labels))\n",
        "\n",
        "all_X_full  = full_ds.trials   # list of numpy arrays, each full-trial segments\n",
        "all_y_full  = full_ds.labels\n",
        "all_X_early = early_ds.trials  # same-length list, but each truncated to ≤800 ms\n",
        "all_y_early = early_ds.labels\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 8) 8-fold cross-validation over all trials (mixing T+E)\n",
        "# --------------------------------------------------\n",
        "kf         = KFold(n_splits=8, shuffle=True, random_state=42)\n",
        "device     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bs, ne, lr = 8, 35, 3e-3\n",
        "patience   = 50\n",
        "fold_accs  = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(all_X_full), 1):\n",
        "    print(f\"\\n=== Fold {fold}/8 ===\")\n",
        "\n",
        "    # --- Build training split: full segments of those trials ---\n",
        "    X_train = [all_X_full[i] for i in train_idx]\n",
        "    y_train = [all_y_full[i] for i in train_idx]\n",
        "\n",
        "    # --- Build validation split: early (≤800 ms) segments of held-out trials ---\n",
        "    X_val = [all_X_early[i] for i in val_idx]\n",
        "    y_val = [all_y_early[i] for i in val_idx]\n",
        "\n",
        "    # --- Compute normalization stats on training data only ---\n",
        "    all_train_segments = np.concatenate(X_train, axis=0)  # shape = (sum of T_i, 231)\n",
        "    μ = all_train_segments.mean(axis=0, keepdims=True)\n",
        "    σ = all_train_segments.std(axis=0, keepdims=True) + 1e-6\n",
        "    normalize = lambda seqs: [(s - μ) / σ for s in seqs]\n",
        "\n",
        "    X_train_norm = normalize(X_train)\n",
        "    X_val_norm   = normalize(X_val)\n",
        "\n",
        "    # --- Wrap in DataLoaders ---\n",
        "    class TrialDataset(Dataset):\n",
        "        def __init__(self, X, y):\n",
        "            self.X = X\n",
        "            self.y = y\n",
        "        def __len__(self):\n",
        "            return len(self.X)\n",
        "        def __getitem__(self, i):\n",
        "            return self.X[i], self.y[i]\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TrialDataset(X_train_norm, y_train),\n",
        "        batch_size=bs,\n",
        "        shuffle=True,\n",
        "        collate_fn=pad_collate\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        TrialDataset(X_val_norm, y_val),\n",
        "        batch_size=bs,\n",
        "        shuffle=False,\n",
        "        collate_fn=pad_collate\n",
        "    )\n",
        "\n",
        "    # --- Instantiate model with appropriate max sequence length ---\n",
        "    max_T = max(\n",
        "        max(tr.shape[0] for tr in X_train_norm),\n",
        "        max(tr.shape[0] for tr in X_val_norm)\n",
        "    )\n",
        "    model = EEGTransformerClassifier(\n",
        "        feature_dim=231,\n",
        "        d_model=16,\n",
        "        nhead=2,\n",
        "        num_layers=2,\n",
        "        num_classes=len(tasks),\n",
        "        dim_ff=256,\n",
        "        dropout=0.1,\n",
        "        max_seq_len=max_T\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    wait = patience\n",
        "    ckpt_path = f\"fold{fold}.pt\"\n",
        "\n",
        "    # --- Training loop ---\n",
        "    for ep in range(1, ne + 1):\n",
        "        model.train()\n",
        "        total_loss, total_correct, total_samples = 0.0, 0, 0\n",
        "\n",
        "        for Xb, yb, mb in train_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            mb = mb.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(Xb, mb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            total_correct += (preds == yb).sum().item()\n",
        "            total_samples += yb.size(0)\n",
        "            total_loss += loss.item() * yb.size(0)\n",
        "\n",
        "        train_loss = total_loss / total_samples\n",
        "        train_acc  = 100.0 * total_correct / total_samples\n",
        "\n",
        "        # --- Validation on \"early\" segments ---\n",
        "        model.eval()\n",
        "        correct, count = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for Xb, yb, mb in val_loader:\n",
        "                Xb = Xb.to(device)\n",
        "                yb = yb.to(device)\n",
        "                mb = mb.to(device)\n",
        "                preds = model(Xb, mb).argmax(dim=1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "                count += yb.size(0)\n",
        "        val_acc = 100.0 * correct / count\n",
        "\n",
        "        # --- Early stopping logic ---\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc, wait = val_acc, patience\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "        else:\n",
        "            wait -= 1\n",
        "            if wait == 0:\n",
        "                print(f\"⟹ Early stop @ epoch {ep}\")\n",
        "                break\n",
        "\n",
        "        # --- Print stats at ep 1 and every 5 epochs ---\n",
        "        if ep == 1 or ep % 5 == 0:\n",
        "            print(f\"Ep{ep:02d}: tr-loss {train_loss:.4f} | \"\n",
        "                  f\"tr-acc {train_acc:.2f}% | early-val-acc {val_acc:.2f}%\")\n",
        "\n",
        "    # --- Final evaluation for this fold ---\n",
        "    model.load_state_dict(torch.load(ckpt_path))\n",
        "    model.eval()\n",
        "    correct, count = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb, mb in val_loader:\n",
        "            Xb = Xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            mb = mb.to(device)\n",
        "            preds = model(Xb, mb).argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            count += yb.size(0)\n",
        "    fold_acc = 100.0 * correct / count\n",
        "    fold_accs.append(fold_acc)\n",
        "    print(f\"♦ Fold {fold} final early-800ms acc: {fold_acc:.2f}%\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# 9) Summary of CV results\n",
        "# --------------------------------------------------\n",
        "print(f\"\\nMean ± SD early-800ms accuracy over 8 folds: \"\n",
        "      f\"{np.mean(fold_accs):.2f}% ± {np.std(fold_accs):.2f}%\")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def dataset_stats(ds, name=\"dataset\"):\n",
        "    seg_counts = [trial.shape[0] for trial in ds.trials]\n",
        "    n_trials = len(seg_counts)\n",
        "    n_segments = int(np.sum(seg_counts))\n",
        "\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"-\" * len(name))\n",
        "    print(f\"Trials               : {n_trials}\")\n",
        "    print(f\"Segments             : {n_segments}\")\n",
        "    print(f\"[CLS] tokens (1/trial): {n_trials}\")\n",
        "    print(f\"Total tokens         : {n_segments + n_trials}\")\n",
        "    print(f\"Segments/trial       : \"\n",
        "          f\"mean {np.mean(seg_counts):.1f}, \"\n",
        "          f\"min {np.min(seg_counts)}, \"\n",
        "          f\"max {np.max(seg_counts)}\")\n",
        "\n",
        "    return n_trials, n_segments\n",
        "\n",
        "# --- Use main code's variables: data_dir, subject_id, tasks ---\n",
        "# Instantiate datasets\n",
        "full_ds = EEGGraphDataset(data_dir, subject_id, test_early=False)\n",
        "early_ds_800 = EEGGraphDataset(data_dir, subject_id, test_early=True, early_ms=800)\n",
        "\n",
        "# --- Overall stats ---\n",
        "train_trials, train_segs = dataset_stats(full_ds, \"TRAIN  (all segments)\")\n",
        "test_800_trials, test_800_segs = dataset_stats(early_ds_800, \"TEST   (segments ending ≤800 ms)\")\n",
        "\n",
        "# --- CV split stats ---\n",
        "cv_train_trials, cv_train_segs, cv_test_trials, cv_test_segs_800 = [], [], [], []\n",
        "for tr_idx, val_idx in KFold(n_splits=10, shuffle=True, random_state=42).split(full_ds.trials):\n",
        "    # Training\n",
        "    train_trials = [full_ds.trials[i] for i in tr_idx]\n",
        "    train_segs = int(np.sum([t.shape[0] for t in train_trials]))\n",
        "    cv_train_trials.append(len(tr_idx))\n",
        "    cv_train_segs.append(train_segs)\n",
        "    # Testing (≤800 ms)\n",
        "    test_trials_800 = [early_ds_800.trials[i] for i in val_idx]\n",
        "    test_segs_800 = int(np.sum([t.shape[0] for t in test_trials_800]))\n",
        "    cv_test_trials.append(len(val_idx))\n",
        "    cv_test_segs_800.append(test_segs_800)\n",
        "\n",
        "# --- Print CV folds ---\n",
        "print(\"\\n8-fold split sizes (train / valid per fold, ≤800 ms)\")\n",
        "for i, (tr_idx, val_idx) in enumerate(KFold(n_splits=8, shuffle=True, random_state=42).split(full_ds.trials), 1):\n",
        "    train_segs = int(np.sum([full_ds.trials[i].shape[0] for i in tr_idx]))\n",
        "    test_segs_800 = int(np.sum([early_ds_800.trials[i].shape[0] for i in val_idx]))\n",
        "    print(f\"Fold {i}: {len(tr_idx):4d} trials ({train_segs:5d} segs) / \"\n",
        "          f\"{len(val_idx):3d} trials ({test_segs_800:4d} segs ≤800 ms)\")\n",
        "\n",
        "# --- Print CV means ---\n",
        "print(\"\\nCV Mean Statistics:\")\n",
        "print(f\"CV Training Trials      : {np.mean(cv_train_trials):.0f}\")\n",
        "print(f\"CV Training Segments    : {np.mean(cv_train_segs):.0f}\")\n",
        "print(f\"CV Testing Trials       : {np.mean(cv_test_trials):.0f}\")\n",
        "print(f\"CV Testing Segments (≤800 ms) : {np.mean(cv_test_segs_800):.0f}\")"
      ]
    }
  ]
}
